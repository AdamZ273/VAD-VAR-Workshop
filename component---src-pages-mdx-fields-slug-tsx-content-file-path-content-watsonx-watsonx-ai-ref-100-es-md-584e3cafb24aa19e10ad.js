"use strict";(self.webpackChunkvad_var_site=self.webpackChunkvad_var_site||[]).push([[9879],{1333:function(e,n,a){a.r(n),a.d(n,{Head:function(){return E},default:function(){return b}});var t=a(1151),o=a(7294);function l(e){const n=Object.assign({section:"section",h1:"h1",ul:"ul",li:"li",p:"p",strong:"strong"},(0,t.ah)(),e.components);return o.createElement(n.section,{className:"heading","data-heading-rank":"1","aria-labelledby":"glosario-de-términos-de-ia"},o.createElement(n.h1,{id:"glosario-de-términos-de-ia"},"Glosario de términos de IA"),"\n",o.createElement(n.ul,null,"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,"Los ",o.createElement(n.strong,null,"modelos básicos")," normalmente se construyen utilizando un tipo específico de arquitectura de red neuronal, llamado transformador, que está diseñado para generar secuencias de elementos de datos relacionados (por ejemplo, una oración)."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"IA generativa")," se refiere a un conjunto de algoritmos de IA que pueden generar nuevos resultados (como texto, imágenes, código o audio) en función de los datos de entrenamiento, a diferencia de los sistemas de IA tradicionales que están diseñados para reconocer patrones y hacer predicciones. . A veces, la IA que impulsa estas soluciones se denomina decodificadores."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"La alucinación")," es un fenómeno bien conocido en los modelos de lenguaje grandes (LLM, por sus siglas en inglés) en el que el sistema proporciona una respuesta objetivamente incorrecta, irrelevante o sin sentido debido a limitaciones en sus datos de entrenamiento y arquitectura; Lo más preocupante es que la respuesta alucinada suena plausible."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,"Un ",o.createElement(n.strong,null,"modelo de lenguaje grande (LLM)")," es un tipo de modelo de aprendizaje automático que se ha entrenado en grandes cantidades de texto sin etiquetar mediante aprendizaje autosupervisado y puede realizar una variedad de tareas de procesamiento del lenguaje natural (NLP) (incluso cuando ese lenguaje es un lenguaje de programación). Los resultados pueden variar desde libros, artículos, publicaciones en redes sociales, conversaciones en línea e incluso código. La arquitectura de un LLM consta de capas de redes neuronales que aprenden a generar el lenguaje de una manera similar a cómo los humanos usan el lenguaje."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"El procesamiento del lenguaje natural (NLP)"),' es la tecnología que brinda a las computadoras la capacidad de comprender texto y palabras habladas de la misma manera que los seres humanos. La PNL combina la lingüística computacional (modelado del lenguaje humano basado en reglas) con modelos estadísticos, de aprendizaje automático y de aprendizaje profundo. Estas tecnologías permiten a las computadoras procesar el lenguaje humano en forma de texto o datos de voz y "comprender" su significado completo, junto con la intención y el sentimiento del hablante o escritor.'),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"Preguntar"),": entrada y consulta que los usuarios o programas utilizan para interactuar con los modelos básicos para que puedan responder con resultados útiles/deseables. Un mensaje puede ser una simple pregunta de PNL o puede ser un texto extenso. La estructura del mensaje es muy importante para obtener respuestas adecuadas de los modelos básicos."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"Ingeniería de avisos"),": la ingeniería de avisos es el proceso de elaboración de texto de aviso para lograr el mejor efecto en un modelo y parámetros determinados."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"Modelo solo decodificador"),": modelos diseñados explícitamente para casos de uso de IA generativa; representa las arquitecturas utilizadas en GPT-3 y otros modelos de lenguajes grandes populares."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"Modelo de solo codificador"),": modelos con la mejor relación costo-rendimiento para casos de uso no generativos, pero que requieren datos etiquetados de tareas específicas para realizar ajustes."),"\n"),"\n",o.createElement(n.li,null,"\n",o.createElement(n.p,null,o.createElement(n.strong,null,"Modelo codificador-decodificador")," : modelos que admiten casos de uso tanto generativos como no generativos. Estos tienen la mejor relación costo-rendimiento para casos de uso generativo cuando el insumo es grande pero el resultado generado es pequeño."),"\n"),"\n"))}var r=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,t.ah)(),e.components);return n?o.createElement(n,e,o.createElement(l,e)):l(e)},s=a(4184),i=a.n(s),c=a(4690),u=a(1140),d=a(2565),m=a(8531),p=a(3383),g=a(7315);const f=e=>{const{data:{mdx:{tableOfContents:{items:n},frontmatter:{toc:a=!0,title:t,timeToComplete:l,updated:r}}},children:s}=e,c=(0,o.useRef)(null),{0:f}=(0,o.useState)(""),E=(null===a||a)&&n;return o.createElement(o.Fragment,null,o.createElement(u.Z,{timeToComplete:l,updated:r},n[0].title||t||""),o.createElement(m.Z,{className:g.YS},o.createElement("article",{className:i()(g.Y2,!E&&g.ey),ref:c},o.createElement(d.Z,{components:{h1:()=>null}},s)),E&&o.createElement(p.Z,{itemsList:n,maxDepth:2,currSection:f})))},E=e=>{const{location:{pathname:n},data:{mdx:{frontmatter:{title:a},tableOfContents:{items:t}}}}=e;return o.createElement(c.Z,{pathname:n,title:a||t[0].title||void 0})};function b(e){return o.createElement(f,e,o.createElement(r,e))}},2565:function(e,n,a){a.d(n,{Z:function(){return s}});var t=a(7294),o=a(1151),l=a(7563);const r={table:l.y6,a:l.IW,blockquote:l.R4,SubHeader:l.bU,img:l.fb,code:l.dn,QuizAlert:l.SA,Danger:l.b0,Warning:l.v3,CopyText:l.O5};var s=(0,t.memo)((function(e){let{children:n,components:a={}}=e;return t.createElement(o.Zo,{components:{...r,...a}},n)}))},3383:function(e,n,a){a.d(n,{Z:function(){return c}});var t=a(7294),o=a(7500),l=a(4184),r=a.n(l),s=a(6488);const i=function(e,n,a){return void 0===n&&(n=[]),void 0===a&&(a=0),e.forEach((e=>{const{title:t,url:o,items:l}=e;n.splice(n.length,0,{depth:a,title:t,url:o}),l&&l.length>0&&i(l,n,a+1)})),n};var c=e=>{const{itemsList:n}=e,a=(0,t.useMemo)((()=>i(n[0].items||[])),[n]),l=(0,s.Z)("h1[id],h2[id]",{threshold:[0,1],rootMargin:"-48px 0px -90% 0px"});return a.length<1?null:t.createElement("nav",{className:"TableOfContents-module--toc--54d35"},t.createElement("div",{className:"TableOfContents-module--tocStack--90609"},t.createElement("h6",{className:"TableOfContents-module--tocHeader--05956"},t.createElement(o.rU,{to:"#",replace:!0},"On this page")),a.map(((e,n)=>{let{title:a,url:o}=e;return t.createElement("a",{className:r()("TableOfContents-module--link--b292b",l===o.substring(1)&&"TableOfContents-module--activeItem--3869f"),key:n,href:o},a)}))))}},7315:function(e,n,a){a.d(n,{Y2:function(){return t},YS:function(){return l},ey:function(){return o}});var t="{mdx-fields__slug}-module--article--e3d5a",o="{mdx-fields__slug}-module--noToc--82387",l="{mdx-fields__slug}-module--wrapper--58e72"},1151:function(e,n,a){a.d(n,{Zo:function(){return s},ah:function(){return l}});var t=a(7294);const o=t.createContext({});function l(e){const n=t.useContext(o);return t.useMemo((()=>"function"==typeof e?e(n):{...n,...e}),[n,e])}const r={};function s({components:e,children:n,disableParentContext:a}){let s;return s=a?"function"==typeof e?e({}):e||r:l(e),t.createElement(o.Provider,{value:s},n)}}}]);
//# sourceMappingURL=component---src-pages-mdx-fields-slug-tsx-content-file-path-content-watsonx-watsonx-ai-ref-100-es-md-584e3cafb24aa19e10ad.js.map